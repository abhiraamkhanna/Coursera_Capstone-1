
The data is derived from the Data-Collision.csv file which records Seattleâ€™s car accidents. It takes into account 37 different attributes and is composed of over 190,000+ rows. Obviously, there are a lot of outliers and NaN values throughout the dataset. Regardless, we must first decide what features to use: categorical data include the type of collision, collision codes, whether or not the pedestrian was granted right of way, road conditions, and weather conditions. The quantitative data includes # injuries, pedCounts, vehicle counts, and fatalities. The aforementioned quantitative and categorical features will be consolidated onto a dataframe and used as our feature set, and from there, we will clean the data. As I said before, this dataframe will have outliers and null values and will not be balanced at all: we must eliminate all these nuances to build a better predicting model. Next, we will have to consider a variety of classification and regression techniques, ranging from KNN, decision trees, logistic regression, and more, to find the relation between those features and the end target variable: the severity index of an accident. After finding that relation, we will be able to predict the severity index of a potential car accident based on provided values for our feature set.
